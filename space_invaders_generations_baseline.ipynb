{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "space_invaders_generations_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yAyrS19lVaV",
        "outputId": "1b7fbd34-5bf6-4c61-cfbf-68d3a3696396"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gym\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "#from torch_deep_q_model import DeepQNetwork, Agent\n",
        "#from utils import plotLearning\n",
        "import numpy as np\n",
        "import random\n",
        "from gym import wrappers\n",
        "import math\n",
        "print(T.cuda.is_available())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WMqgQZ5lBE4"
      },
      "source": [
        "class DeepQNetwork(nn.Module):\n",
        "    def __init__(self, ALPHA):\n",
        "        super(DeepQNetwork, self).__init__()\n",
        "        #self.conv1 = nn.Conv2d(3, 32, 8, stride=4, padding=1)\n",
        "        self.conv1 = nn.Conv2d(1, 32, 8, stride=4, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
        "        #self.fc1 = nn.Linear(128*23*16, 512)\n",
        "        self.fc1 = nn.Linear(128*19*8, 512)\n",
        "        self.fc2 = nn.Linear(512, 6)\n",
        "        #self.optimizer = optim.SGD(self.parameters(), lr=self.ALPHA, momentum=0.9)\n",
        "        self.optimizer = optim.RMSprop(self.parameters(), lr=ALPHA)\n",
        "        self.loss = nn.MSELoss()\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, observation):\n",
        "        observation = T.Tensor(observation).to(self.device)\n",
        "        #observation = observation.view(-1, 3, 210, 160).to(self.device)\n",
        "        observation = observation.view(-1, 1, 185, 95)\n",
        "        observation = F.relu(self.conv1(observation))\n",
        "        observation = F.relu(self.conv2(observation))\n",
        "        observation = F.relu(self.conv3(observation))\n",
        "        #observation = observation.view(-1, 128*23*16).to(self.device)\n",
        "        observation = observation.view(-1, 128*19*8)\n",
        "        observation = F.relu(self.fc1(observation))\n",
        "        actions = self.fc2(observation)\n",
        "        return actions\n",
        "\n",
        "class Agent(object):\n",
        "    def __init__(self, gamma, epsilon, alpha,\n",
        "                 maxMemorySize, epsEnd=0.05,\n",
        "                 replace=10000, actionSpace=[0,1,2,3,4,5]):\n",
        "        self.GAMMA = gamma\n",
        "        self.EPSILON = epsilon\n",
        "        self.EPS_END = epsEnd\n",
        "        self.ALPHA = alpha\n",
        "        self.actionSpace = actionSpace\n",
        "        self.memSize = maxMemorySize\n",
        "        self.steps = 0\n",
        "        self.learn_step_counter = 0\n",
        "        self.memory = []\n",
        "        self.memCntr = 0\n",
        "        self.replace_target_cnt = replace\n",
        "        self.Q_eval = DeepQNetwork(alpha)\n",
        "        self.Q_next = DeepQNetwork(alpha)\n",
        "        self.score = 0\n",
        "\n",
        "    def storeTransition(self, state, action, reward, state_):\n",
        "        if self.memCntr < self.memSize:\n",
        "            self.memory.append([state, action, reward, state_])\n",
        "        else:\n",
        "            self.memory[self.memCntr%self.memSize] = [state, action, reward, state_]\n",
        "        self.memCntr += 1\n",
        "\n",
        "    def chooseAction(self, observation):\n",
        "        rand = np.random.random()\n",
        "        actions = self.Q_eval.forward(observation)\n",
        "        if rand < 1 - self.EPSILON:\n",
        "            action = T.argmax(actions[1]).item()\n",
        "        else:\n",
        "            action = np.random.choice(self.actionSpace)\n",
        "        self.steps += 1\n",
        "        return action\n",
        "\n",
        "    def chooseActionPBT(self, observation, policy_net_parent=None):\n",
        "        rand = np.random.random()\n",
        "        eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * self.steps / EPS_DECAY)\n",
        "        self.steps += 1\n",
        "        if rand > eps_threshold:\n",
        "            return T.argmax(self.Q_eval.forward(observation)[1]).item() #self.Q_eval.forward(observation).max(1)[1].view(1, 1)\n",
        "        else:\n",
        "            if policy_net_parent and random.random() < 0.5:\n",
        "                return T.argmax(policy_net_parent(observation)[1]).item() #policy_net_parent(observation).max(1)[1].view(1, 1)\n",
        "            else:\n",
        "                return np.random.choice(self.actionSpace) #T.tensor([[random.randrange(self.actionSpace)]], device=device, dtype=T.long)\n",
        "\n",
        "    def learn(self, batch_size):\n",
        "        self.Q_eval.optimizer.zero_grad()\n",
        "        if self.replace_target_cnt is not None and \\\n",
        "           self.learn_step_counter % self.replace_target_cnt == 0:\n",
        "            self.Q_next.load_state_dict(self.Q_eval.state_dict())\n",
        "\n",
        "        if self.memCntr+batch_size < self.memSize:\n",
        "            memStart = int(np.random.choice(range(self.memCntr)))\n",
        "        else:\n",
        "            memStart = int(np.random.choice(range(self.memSize-batch_size-1)))\n",
        "        miniBatch=self.memory[memStart:memStart+batch_size]\n",
        "        memory = np.array(miniBatch)\n",
        "\n",
        "        # convert to list because memory is an array of numpy objects\n",
        "        Qpred = self.Q_eval.forward(list(memory[:,0][:])).to(self.Q_eval.device)\n",
        "        Qnext = self.Q_next.forward(list(memory[:,3][:])).to(self.Q_eval.device)\n",
        "\n",
        "        maxA = T.argmax(Qnext, dim=1).to(self.Q_eval.device)\n",
        "        rewards = T.Tensor(list(memory[:,2])).to(self.Q_eval.device)\n",
        "        Qtarget = Qpred.clone()\n",
        "        indices = np.arange(batch_size)\n",
        "        Qtarget[indices,maxA] = rewards + self.GAMMA*T.max(Qnext[1])\n",
        "\n",
        "        if self.steps > 500:\n",
        "            if self.EPSILON - 1e-4 > self.EPS_END:\n",
        "                self.EPSILON -= 1e-4\n",
        "            else:\n",
        "                self.EPSILON = self.EPS_END\n",
        "\n",
        "        #Qpred.requires_grad_()\n",
        "        loss = self.Q_eval.loss(Qtarget, Qpred).to(self.Q_eval.device)\n",
        "        loss.backward()\n",
        "        self.Q_eval.optimizer.step()\n",
        "        self.learn_step_counter += 1\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYstvwrHlIFQ"
      },
      "source": [
        "def plotLearning(x, scores, epsilons, filename, lines=None):\n",
        "    fig=plt.figure()\n",
        "    ax=fig.add_subplot(111, label=\"1\")\n",
        "    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
        "\n",
        "    ax.plot(x, epsilons, color=\"C0\")\n",
        "    ax.set_xlabel(\"Game\", color=\"C0\")\n",
        "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
        "    ax.tick_params(axis='x', colors=\"C0\")\n",
        "    ax.tick_params(axis='y', colors=\"C0\")\n",
        "\n",
        "    N = len(scores)\n",
        "    running_avg = np.empty(N)\n",
        "    for t in range(N):\n",
        "        running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n",
        "\n",
        "    ax2.scatter(x, running_avg, color=\"C1\")\n",
        "    #ax2.xaxis.tick_top()\n",
        "    ax2.axes.get_xaxis().set_visible(False)\n",
        "    ax2.yaxis.tick_right()\n",
        "    #ax2.set_xlabel('x label 2', color=\"C1\")\n",
        "    ax2.set_ylabel('Score', color=\"C1\")\n",
        "    #ax2.xaxis.set_label_position('top')\n",
        "    ax2.yaxis.set_label_position('right')\n",
        "    #ax2.tick_params(axis='x', colors=\"C1\")\n",
        "    ax2.tick_params(axis='y', colors=\"C1\")\n",
        "\n",
        "    if lines is not None:\n",
        "        for line in lines:\n",
        "            plt.axvline(x=line)\n",
        "\n",
        "    plt.savefig(filename)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t39-OAeVGJ4S"
      },
      "source": [
        "Training a single agent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VWvwkffclLX0",
        "outputId": "e9f17eb4-2520-4c25-9d85-79f287715105"
      },
      "source": [
        "    env = gym.make('SpaceInvaders-v0')\n",
        "    brain = Agent(gamma=0.95, epsilon=1.0,\n",
        "                  alpha=0.003, maxMemorySize=5000,\n",
        "                  replace=None)\n",
        "    while brain.memCntr < brain.memSize:\n",
        "        observation = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            # 0 no action, 1 fire, 2 move right, 3 move left, 4 move right fire, 5 move left fire\n",
        "            action = env.action_space.sample()\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            if done and info['ale.lives'] == 0:\n",
        "                reward = -100\n",
        "            brain.storeTransition(np.mean(observation[15:200,30:125], axis=2), action, reward,\n",
        "                                np.mean(observation_[15:200,30:125], axis=2))\n",
        "            observation = observation_\n",
        "    print('done initializing memory')\n",
        "\n",
        "    scores = []\n",
        "    epsHistory = []\n",
        "    numGames = 50\n",
        "    batch_size=32\n",
        "    # uncomment the line below to record every episode.\n",
        "    env = wrappers.Monitor(env, \"tmp/space-invaders-1\", video_callable=lambda episode_id: True, force=True)\n",
        "    for i in range(numGames):\n",
        "        print('starting game ', i+1, 'epsilon: %.4f' % brain.EPSILON)\n",
        "        epsHistory.append(brain.EPSILON)\n",
        "        done = False\n",
        "        observation = env.reset()\n",
        "        frames = [np.sum(observation[15:200,30:125], axis=2)]\n",
        "        score = 0\n",
        "        lastAction = 0\n",
        "        while not done:\n",
        "            if len(frames) == 3:\n",
        "                action = brain.chooseAction(frames)\n",
        "                frames = []\n",
        "            else:\n",
        "                action = lastAction\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            frames.append(np.sum(observation_[15:200,30:125], axis=2))\n",
        "            if done and info['ale.lives'] == 0:\n",
        "                reward = -100\n",
        "            brain.storeTransition(np.mean(observation[15:200,30:125], axis=2), action, reward,\n",
        "                                  np.mean(observation_[15:200,30:125], axis=2))\n",
        "            observation = observation_\n",
        "            brain.learn(batch_size)\n",
        "            lastAction = action\n",
        "            #env.render(\n",
        "        scores.append(score)\n",
        "        print('score:',score)\n",
        "    x = [i+1 for i in range(numGames)]\n",
        "    fileName = str(numGames) + 'Games' + 'Gamma' + str(brain.GAMMA) + \\\n",
        "               'Alpha' + str(brain.ALPHA) + 'Memory' + str(brain.memSize)+ '.png'\n",
        "    plotLearning(x, scores, epsHistory, fileName)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done initializing memory\n",
            "starting game  1 epsilon: 1.0000\n",
            "score: 75.0\n",
            "starting game  2 epsilon: 1.0000\n",
            "score: 140.0\n",
            "starting game  3 epsilon: 1.0000\n",
            "score: 105.0\n",
            "starting game  4 epsilon: 0.9756\n",
            "score: 130.0\n",
            "starting game  5 epsilon: 0.9100\n",
            "score: 65.0\n",
            "starting game  6 epsilon: 0.8411\n",
            "score: 80.0\n",
            "starting game  7 epsilon: 0.7725\n",
            "score: 160.0\n",
            "starting game  8 epsilon: 0.6930\n",
            "score: 45.0\n",
            "starting game  9 epsilon: 0.6411\n",
            "score: 45.0\n",
            "starting game  10 epsilon: 0.6015\n",
            "score: 20.0\n",
            "starting game  11 epsilon: 0.5624\n",
            "score: 335.0\n",
            "starting game  12 epsilon: 0.4485\n",
            "score: 10.0\n",
            "starting game  13 epsilon: 0.4136\n",
            "score: 255.0\n",
            "starting game  14 epsilon: 0.3152\n",
            "score: 25.0\n",
            "starting game  15 epsilon: 0.2751\n",
            "score: 235.0\n",
            "starting game  16 epsilon: 0.2052\n",
            "score: 225.0\n",
            "starting game  17 epsilon: 0.0909\n",
            "score: 275.0\n",
            "starting game  18 epsilon: 0.0500\n",
            "score: 590.0\n",
            "starting game  19 epsilon: 0.0500\n",
            "score: 395.0\n",
            "starting game  20 epsilon: 0.0500\n",
            "score: 220.0\n",
            "starting game  21 epsilon: 0.0500\n",
            "score: 90.0\n",
            "starting game  22 epsilon: 0.0500\n",
            "score: 185.0\n",
            "starting game  23 epsilon: 0.0500\n",
            "score: 125.0\n",
            "starting game  24 epsilon: 0.0500\n",
            "score: 145.0\n",
            "starting game  25 epsilon: 0.0500\n",
            "score: 120.0\n",
            "starting game  26 epsilon: 0.0500\n",
            "score: 15.0\n",
            "starting game  27 epsilon: 0.0500\n",
            "score: 15.0\n",
            "starting game  28 epsilon: 0.0500\n",
            "score: 15.0\n",
            "starting game  29 epsilon: 0.0500\n",
            "score: 260.0\n",
            "starting game  30 epsilon: 0.0500\n",
            "score: 215.0\n",
            "starting game  31 epsilon: 0.0500\n",
            "score: 285.0\n",
            "starting game  32 epsilon: 0.0500\n",
            "score: 180.0\n",
            "starting game  33 epsilon: 0.0500\n",
            "score: 180.0\n",
            "starting game  34 epsilon: 0.0500\n",
            "score: 180.0\n",
            "starting game  35 epsilon: 0.0500\n",
            "score: 25.0\n",
            "starting game  36 epsilon: 0.0500\n",
            "score: 30.0\n",
            "starting game  37 epsilon: 0.0500\n",
            "score: 180.0\n",
            "starting game  38 epsilon: 0.0500\n",
            "score: 310.0\n",
            "starting game  39 epsilon: 0.0500\n",
            "score: 260.0\n",
            "starting game  40 epsilon: 0.0500\n",
            "score: 180.0\n",
            "starting game  41 epsilon: 0.0500\n",
            "score: 120.0\n",
            "starting game  42 epsilon: 0.0500\n",
            "score: 155.0\n",
            "starting game  43 epsilon: 0.0500\n",
            "score: 120.0\n",
            "starting game  44 epsilon: 0.0500\n",
            "score: 30.0\n",
            "starting game  45 epsilon: 0.0500\n",
            "score: 180.0\n",
            "starting game  46 epsilon: 0.0500\n",
            "score: 105.0\n",
            "starting game  47 epsilon: 0.0500\n",
            "score: 10.0\n",
            "starting game  48 epsilon: 0.0500\n",
            "score: 0.0\n",
            "starting game  49 epsilon: 0.0500\n",
            "score: 5.0\n",
            "starting game  50 epsilon: 0.0500\n",
            "score: 150.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAEGCAYAAAA0UdFjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZgU5bn38W/PMDCsxTJsMqPFLoiIMioyiaKoQcvtJAbFLG4JOYlxi1nKnHO0jnmTU0lUokYTiRo0UQkxJhLbuK8BRRYVBRRZChhUdlpZBAb6/aN6tBmmZ7qHrqlefp/rmmu6q6u770qQm+ep+7mfSDweR0REJFeVhB2AiIhIU5SoREQkpylRiYhITlOiEhGRnKZEJSIiOa1N2AFkqqKiIm6aZthhiIjklfnz52+Mx+M9w46jJfIuUZmmybx588IOQ0Qkr0QikVVhx9BSmvoTEZGcpkQlIiI5TYlKRERymhKViIjkNCUqERHJaYFV/Zl29D7gLGC951ojGnk9AtwGnAnsAC7xXGtBUPGISCMWzoDnboJYLRiVMP4GGDkx7KhE9hPkiGoaMKGJ188ABid+JgO/CzAWkeKwcAZMGQFOV//3whlNn/vPqyC2Boj7v/95VdPvEQlBYCMqz7VeNu2o2cQp5wIPeK4VB14z7WhX04729VzrwyDimett5pWlGzJ6T0lJhPNG9cOs6BhESCLZVZ949uz0n9cnHmh8lPTcTZ+fW2/PTv+4RlWSQ8Jc8NsPWJP0vDZx7IBEZdrRyfijLkq2727Rly1YtYU7XliW0Xvicbj33yu5Y9LRjBvaq0XfK9JqMk08sdrGPyfVcZGQ5EVnCs+1pgJTAaqfvbFFOz1+56SBfOekgRm9p3bLDr79wHwumzaX688Yxre+2J9IJNKSrxcJXqaJx6hMTPs1clwkh4RZ9bcWqEp6Xpk4ljMqu3Xgb989gQkj+vDzJ5Zw3Yy3+HTP3rDDEmlcqgST6vj4G6Cs/f7Hytr7x0VySJiJaibwTdOORkw7OgaIBXV/6mB0aNuGOy86hh+cNoRH31jLBVNfY93Hn4YdlsiBMk08IyfC2beDUQVE/N9n3+4fT1WUkUmxhkiWROLxFs2kNcu0ow8D44AKYB1wI1AG4LnW7xPl6b/FrwzcAVzquVaz3Warq6vjYTWlffKdj/jBjDfp1K4N9158LEdWGqHEIZJSNsrNGxZlgJ/wjroI3nrowOP1yU1yWiQSmR+Px6vDjqMlAktUQQkzUQG8+9HHXPbHuXQqb8NT15yoe1ZSeKaMaPzeVaQU4o1MfRtVcO07wcclByWfE5U6U2To8D5d+OGXhrJ03TZezLDcXSQvpCq+aCxJNXW+SJYoUbXA2UcdQp8u5Ux9aUXYoYhkX6rii0hpZueLZIkSVQuUlZZw2RdMXl2xibdrY2GHI5JdqYoyRl+iKkEJhRJVC0067lA6t2vD1Fc0qpICk6oa8KxbU1cJigRIxRQH4RdPLOHef6/kxR+Oo6p7h7DDERFJScUURerSGpMIcN+slWGHItI0rX+SPKZEdRD6Gu0556hD+MvcNcR27Ak7HJHGqUu65DklqoP07RMHsGP3Xv48Z1XYoYg0rqlmtSJ5QInqIA3r24UvDq5g2myPXXXqAyg5SF3SJc8pUWXBd04cyIZPdvHYGx+EHYrIgTJtViuSY5SosqBmUA+G9+3C1FdWsG9fflVRShFQl3TJc0pUWRCJRJh84gCWrd/Gi0vXhx2OyP6a6pIukgfyYuPEfGCN7MuvnnyXqS+v4JTDe4cdjsj+Rk5UYpK8pUSVJWWlJVxSY/KLJ95l0QcxjjhEW4CISB5xjPuAs4D1OLERiWOjgN8D5UAd8D2c2Os4RgS4DTgTf5umS3BiC4IKTVN/WXRB9aG0Lytl2iwv7FCkkOXq4t1M48rV6yhe0/D3B0z2K+B/cWKjgBsSzwHOAAYnfiYDvwsyMCWqLDI6lPGV0f147K0P2LRtV9jhSCHK1cW7mcaVq9dRzJzYy8DmBkfjQJfEYwOoL20+F3gAJxbHib0GdMUx+gYVmhJVll0ytj+76/bx0JzVYYcihShXF+9mGleuXkcBu+6EthU4xrykn8lpvO0a4Nc4xhrgZuD6xPF+QPLumrWJY4HQPaosG9SrEycO6cmfXlvFd04aSNs2+reAZFGuLt5tKq6FM/wEFKv1126NvyF3r6OA3fLq7o03z96VaVPa7wLX4sT+hmNMBO4FTs1+dE3T36IBuLTGZP0nu/jXOx+GHYoUmlxdvJvq+9t3a3yKr323zD5HwnIx8Gji8V+B4xKP1wJVSedVJo4FQokqACcN7smAio7cp6IKybZcXbybKi5ofIov+fXk88O+DmnoA+CkxONTgPcTj2cC38QxIjjGGCCGEwvsX+ZKVAEoKYlwSY3JW2u2smD1lrDDkUKSq4t3U8W1M8Wf/51bcvM6ipljPAy8CgzFMWpxjMuBbwO34BhvAb/Ar/ADeAJYASwD/gB8L8jQtHFiQLbvqmPM/z3HuKG9uGPS0WGHIxKOKSMS034NGFVw7TutH08R08aJcoCO7dpwQXUV/3r7Qz6KfRp2OCLhyNWpSskrSlQBunisyb54nD+95oUdirSUFqUenFydqpS8ovL0AFV178Cpw3rz0JzVXHnKYMrLSsMOSTJRvyi1/uZ/fcVavYYl1/rLt3HqMygHSSOqgF1SY7Jlxx4eezOwyk3JRCYjpFSLUv/1E3VVEGlFGlEF7IQBPTi8T2f+OMtjYnUVkUgk7JCKV6YjpFSLT3c27DLD510VNHIQyTqNqAIWiUS4tMbk3Y8+4dUVm8IOp7Bkev8o0xFSqkWpqWS7q4Luj4kASlSt4txR/ejesS1/1ALg7GlJU9OmRkiZLEpt373xz6nvqpCNBKOmrSKfUaJqBeVlpVx03KE8u2QdqzftCDucwtCSpqaZtudJtSj1jF+mLrnOVoJR01aRzyhRtZJvnHAYpZEI02Z7YYdSGFrS1DTVmp6mRkgjJ/oLU52t/u/6CrZUJdfZSjBq2iryGRVTtJLeXcqxRvZlxrw1XHvaYDqXl4UdUv5orPu2UZmi40ETo6b6QoeGnwX7F1lA84tSU5VcZyvBtOT6RAqURlSt6NKa/mzbVccj8/Wv4rSlmkobfHrLOh5kOkLKVLa6m6ujg8hnAu31Z9rRCcBtQClwj+daboPXDwXuB7omzrE913qiqc/Ml15/qXz5rlls3r6b568bR0mJStWb1VSvuPE35N6i24Yl8OAnmLNv9x9nEm9jI8mwr0/yVj73+gts6s+0o6XAncBp+Ls/zjXt6EzPtRYnnfbfwAzPtX5n2tHh+B15zaBiygWX1vTnyoff4IX31jN+WO+ww8l9TU2lNdXxIKy/5NOdXkxew5UqLnV0EAGCvUd1HLDMc60VAKYdnQ6cCyQnqjjQJfHYwN/7pKBNGNGHPl3KuW/WSiWqdLTkXk1TC3tbK1k1/J4pI1IXWSgZiTQpyHtU/YDkv2FqE8eSOcDXTTtaiz+aurKxDzLt6GTTjs4z7ei8zdt3BxFrqykrLeGbYw9j1rJNvPfRJ2GHk/tacq8mF0u7VcUn0mJhF1NMAqZ5rlUJnAn8ybSjB8TkudZUz7WqPdeq7t6xbasHmW2Tjj2U8rISps1eGXYoua8lhQ65mBRydQt5kTwQZKJaC1QlPa9MHEt2OTADwHOtV4FyoCLAmHJCt45t+Y+jK3l0wVryfYTYKhqr1GtKLiYFVfGJtFiQiWouMNi0o/1NO9oWuBCY2eCc1cB4ANOODsNPVBsCjClnXFpjsqtuHw+/vjrsUApPLiYF7csk0mJBl6efCfwGv/T8Ps+1fm7a0ZuAeZ5rzUxU+v0B6IRfWPFjz7Webuoz8708PdnX75nDsvXbeOUnJ1NWGvYsbIFRabfIfvK5PD3QRBWEQkpUz7+7jsumzePOi47BGtk37HBEcoP+kRGIfE5U+md8iE4a0ou+RjmPzG+k/FqkGKlrvDRCiSpEpSURvnxMP15auoH1H38adjgi4cvFpQUSOiWqkH3lmEr2xeHvb2ir+qKlDRI/l4tLCyR0SlQhG9CzE6MP68Yj82vJt/uFkgWa6tpfLi4tkNApUeWA80dX8v76bSysjYUdirQ2TXXtLxeXFhQLx7gPx1iPY7zT4PiVOMa7OMYiHONXScevxzGW4Rjv4RhfCjI0JaocYI3sS7s2Jdr+oxhpqmt/Wm8WpmnAhP2OOMbJ+D1aj8KJHQHcnDg+HH9t7BGJ99yFY5QGFZgSVQ7oUl7GhBF9eOzNtXy6Z2/Y4Uhr0lTXgTLtRCLZ4cReBjY3OPpdwMWJ7Uqcsz5x/FxgOk5sF05sJbAMvxF5IJSocsT5oyv5+NM6nl2yLuxQpDVpqktayXUntK3AMeYl/UxO421DgC/iGHNwjJdwjGMTx9NpOp412oo+R4wdWJFYU1XLWSMPCTscaS2p9q/SKEKy7JZXd2+8efauTBf8tgG6A2OAY4EZOMaArAeXRhCSA0pLInzlmEruenEZ6z7+lN5dysMOSVqLNkiU3FULPIoTiwOv4xj78BuHp9N0PGs09ZdDvjK6yNdUaT2RSK75B3AyAI4xBGgLbMRvMH4hjtEOx+gPDAZeDyoIjahySP+KjlQn1lR958QBRCKRsENqPWHvyitS7BzjYWAcUIFj1AI3AvcB9yVK1ncDFydGV4twjBn4O7bXAVfgxAKrBFNT2hwz/fXV2I++zT+uqGFUVdeww2k9U0ak2HK+yq/8EpGDoqa0kjVnjuxLeVlJ8TWq1XoiEUlBiSrHdCkvY8IRfZj55gfFtaZK64lEJAUlqhx0/ugqPv60jqcWfRR2KK1H64lEJAUlqhx0wsAeDKjoyO3Pvc+evfvCDqd1qHWOpEOVoUVJVX85qLQkwk/PHMa3HpjHn19bxaU1/cMOqXVoPZE0RZWhRUsjqhw1flgvvji4ginPLGXz9t1hhyMSPnWaL1pKVDkqEonwP2cNZ/vuvUx5ZmnY4YiEryWVoZoqLAhKVDlsSO/OfP34Q3lwzire++iTsMMRCVemlaHalLJgKFHluGtOHULn8jJuenyRdgCW4pZpZaimCguGElWO69axLdeeOphZyzbxzGJtASJFLNPKUC0iLxiq+ssDXxtzGH+es5qfP7GEk4b2pF2bwDbSFMltmVSGGpUp2nJpEXm+0YgqD5SVlvA/Zw1n1aYdTJvlhR2OSO5prGiiqalCFVnkFSWqPHHSkJ6MP7wXdzy/jA2f7Ao7HJHckapoAhqfKgQVWeQZdU/PIys2bONLv3mZs0cewq0XjAo7HJHckGnn/SLt1K/u6dIqBvTsxH+eNJBH31jL39/QDWERIPOiCRVZ5B0lqjxz9fjBHNe/Oz999B2WrdfaKpGM11epU3/eUaLKM21KS7hj0tF0aFvK9x5cwI7ddWGHJBKuTNdXtbRTvwowQqNElYd6dynnNxeO4v3127jhsUVhhyMSrkzXV7WkU39TXS5SJTAltqxRMUUeu/Xp97j9+WX8+vyRfLW6KuxwRApXqgKM9t2hbuf+HTDK2sNRF8FbDx14PMSta/K5mCLQBb+mHZ0A3AaUAvd4ruU2cs5EwAHiwFuea10UZEyF5OpThzDX28L/PPYOIyu7MrRP57BDEslvC2f4LZZitf49q/E3+IklVaHFzs0HHtuzE+ZPg/jeA4/Xt29q7DtSfbcEN/Vn2tFS4E7gDGA4MMm0o8MbnDMYuB6o8VzrCOCaoOIpRKUlEW6bNIpO7cr43oPz2b5L96tEWqyp6b1MCy0aJql69Z/Z8Dse/4HWdjUhyHtUxwHLPNda4bnWbmA6cG6Dc74N3Om51hYAz7XWBxhPQerVuZzbLxzFio3b+e9/vKPGtSIt1VQT21QFGO27N/5ZkRRtziKljX/H/GlqoNuEIKf++gHJk7q1wPENzhkCYNrRWfjTg47nWk82/CDTjk4GJgOUaBPBA4wdVME144cw5dmlHGt256LjDw07JJH809T6qvopuIZTc7D/rsPQ9D2qhsmoXsoRmNZ2QfhNadsAg4FxQCXwsmlHj/Rca2vySZ5rTQWmAlQ/e6OGDI34/imDmLdqM84/FzGy0mBEPyPskETyS3NNbJtqiNvYvaVDxxx4/LmbGv+OSGnjyUpru4A0E5VpR78M/BLoBUQSP3HPtbo08ba1QHIpWmXiWLJaYI7nWnuAlaYdXYqfuOamF77UKy2J8JsLRnHWHf/muw/O5/Erv4jRvizssETyx/gbGh8dNbe+KlUCS3U8kxFYc99dJNIdUf0KONtzrSUZfPZcYLBpR/vjJ6gLgYYVff8AJgF/NO1oBf5U4IoMvkOS9OjUjt9edAwX3P0qP/rrW9z9jdFEIpGwwxLJD6mm97JZedfUdzQ2AmvNqj/HuA84C1iPExvR4LXrgJuBnjixjThGBL+i+0xgB3AJTmxBUKGlm6jWZZik8FyrzrSj3weewr//dJ/nWotMO3oTMM9zrZmJ10437ehiYC/wI8+1NmXyPbK/0Yd14/ozh/Gzxxdzzysr+faJA8IOSSR/ZLLfVba/ozW+u2nTgN8CD+x31DGqgNOB1UlHz8Cf/RqMX3vwOw6sQciatBb8mnb0NqAP/gjosz0mPNd6NKjAUtGC3+bF43G+9+ACnl68jumTx3CsmaIySUSKRloLfh3DBB7fb0TlGI8APwMeA6oTI6q7gRdxYg8nznkPGIcT+7CZz28PHIoTey+T2NMtT++CP7w7HTg78XNWJl8krScSifDL80dS1a09339oARu3af8qkbyWhXZM153QtgLHmJf0M7nZNznGucBanNhbDV5prKq7XzOfdTbwJvBk4vkoHGNmOrGnNfXnudal6ZwnuaNLeRl3fW00/3HXLK6e/gZ/uux4Skp0v0ok79QvRK4vtEjeGDKDqcJbXt298ebZu9JvoeQYHYCf4g9QssHBX1/7ov8s9iaO0T+dN6Zb9VcJ3AHUJA69AlztuZaK/HPY8EO68N/WMP7nsUXMX71FU4Ai+aiphcjB3tMaCPQH3sIxwK/cXoBjHEd6Vd0N7cGJxRKfVS+t5UbpTv39EZgJHJL4+WfimOS4c47qR0kEZi3bGHYoItISYW306MTexon1womZODETf3rvGJzYR/j54Js4RgTHGAPEmr0/BYtwjIuAUhxjMI5xBzA7nVDSTVQ9Pdf6o+dadYmfaUDPNN8rITI6lDGin8HsZSqmFMlLrbXRo2M8DLwKDMUxanGMy5s4+wn8pUTLgD8A30vjG64EjsAvyHsIiJFmf9d0y9M3mXb068DDieeTAP3NlyfGDqzg3n+vYPuuOjq2C7sZiYhkpKULkTPlxCY187qZ9DgOXJH+ZxulQBQndjLwX5mGlu6I6jJgIvAR8CFwPqACizxRM6gHe/bGed1rZEsCEcltLdnoMdc4sb3APhyjRb3d0q36WwWc05IvkPAda3anbZsSZi/byMlDe4UdjohkKvzFwNmwDXgbx3gG2P7ZUSd2VXNvbDJRmXb0DpqoyvBcq9kvkPCVl5Uy+tBuzNJ9KhEJz6OJn4w1N6JSC4gCUTOoBzc/vZTN23fTvWPbsMMRkWLjxO7HMdqS2N4JeA8ntiedtzaZqDzXuv9gY5PcMHZQBTy9lFeXb8Ia2TfscESk2DjGOOB+wMPfgaMKx7gYJ/Zyc29tburvN55rXWPa0X/SyBSg51q6b5UnRvYz6NyuDbOWb1SiEpEw3AKc/lmfP8cYgl9JPrq5NzY39fenxO+bDyY6CV+b0hKOH9Cd2bmw8HfhjHC3MxCRMJTt14zWiS3FMdLaNK+5qb/5id8v1R8z7Wg3oMpzrYUti1XCMnZgBc8uWc/arTvp17V9OEFkqW+ZiOSdeTjGPcCfE8+/Rpp1EGmtozLt6IumHe1i2tHuwALgD6YdvbVFoUpoagZVACG3U2qqb5mIFLLvAouBqxI/ixPHmpXugl/Dc62PgS8DD3iudTxwagsClRAN6d2Jik7twp3+C6tvmYiErQ1wG07syzixLwO342+q26x0E1Ub0472xe9O8XjLYpSwRSIRxg7swazlm0hnw8xAtFbfMhHJNc8Byfcc2gPPpvPGdBPVTfjbxi/3XGuuaUcHAO9nFKLkhJpBPdjwyS7eX78tnADG3+D3KUsWRN8yEck15Tixz//i8R93SOeN6bZQ+ivw16TnK4CvZBaj5IKxAz+/TzWkd+fWD6C+YEJVfyLFZjuOcQxObAEAjlEN7Gz6Lb50N04cANwGjMFfT/UqcG0iYUkeqeregUO7d2DWsk1cWpPW5prZVxh9y0QkM9cAf8UxPkg87wtckM4b0536ewiYkfjgQ/BHVw83+Q7JWTWDKpizYhN1e/eFHYqIFDrHOBbH6IMTmwscDvwF2AM8CaxM5yPSTVQdPNf6U9LGiX8GylsUtISuZlAPPtlVx9trY2GHIiKF725gd+LxCcBPgTuBLcDUdD4g3V30/mXaURuYjj/1dwHwRGJdFZ5raaOjPHLCgB4AzF6+iaMP7RZyNCJS4EpxYvU54gJgKk7sb8DfcIw30/mAdEdUE4HvAC8AL+Iv0roQmI86rOedHp3aMaxvl3AX/opIsSjFMeoHReOB55NeS2uwlG7VX0h33SUoNQN78MBrq/h0z17Ky9Jacyci0hIPAy/hGBvxq/xeAcAxBgFp3X9ockRl2tEfJz3+aoPXfpFhsJJDagZVsLtuH6+8r1GViATIif0cuA6YBnwBJ1bfbaAEuDKdj2huRHUh8KvE4+tJWksFTMC/KSZ56ISBPTisRweuf3Qhh/epoap7WuvuREQy58Rea+TY0nTf3tw9qkiKx409lzxSXlbKvRcfy+66fVw2bS6xnWlttJm+hTNgyghwuvq/F87I7ueLSNFoLlHFUzxu7LnkmUG9OvH7b4xm5cbtXPHgAvZka11V/VYesTVA/POtPJSsRKQFIk01JzXt6F5gO/7oqT2wo/59QLnnWmltepVN1dXV8XnzVGiYTX+dt4YfPbKQScdV8Yv/OJJI5CAHy1NGJJJUA0YVXPvOwX22iLRIJBKZH4/Hq8OOoyWa2zhR5WBF4KvVVXibtnPnC8sxe3TkOycNPLgP1FYeIvnHMe4DzgLW48RGJI79Gjgbf8HucuBSnNjWxGvXA5cDe4GrcGJPBRVauuuopMBdd9pQzhrZF/fJd3nynQ8P7sO0lYdIPpqGXySX7BlgBE5sJLAUv6gOHGM4frHdEYn33IVjBDawUaISAEpKItz81aMYVdWVa/7yJm/XHkR7JW3lIZJ/nNjLwOYGx57GidUlnr0G1P9r81xgOk5sF05sJbAMOC6o0NJtodQiph2dgN91vRS4x3MtN8V5XwEeAY71XEs3oEJSXlbKH75ZzYTfvMJtz73PPRe3cDpbW3mI5JzrTmhbgWMk//06FSeWVq+9hMvwG8oC9MNPXPVqE8cCEViiMu1oKX7jwdPwL2KuaUdneq61uMF5nYGrgTlBxSLpq+jUjjOP7MOMeWsOrmuFtvIQySm3vLp7482zd7XsX5+O8V9AHfBgVoNKU5BTf8cByzzXWuG51m78hrbnNnLez4BfAp8GGItk4NRhvfl0zz5mL1fXCpGi5xiX4BdZfC2pq8RaoCrprMrEsUAEOfXXD0iuUa4Fjk8+wbSjxwBVnmtFTTv6o1QfZNrRycBkgJLtu1OdJlly/IDudGxbyjOL13PK4b3DDkdEwuIYE4AfAyfhxHYkvTITeAjHuBV/j8LBwOtBhRHoPaqmmHa0BLgVuKS5cz3Xmkpi35LqZ2/UQuOAtWtTyklDe/LcknXs2zeCkhI1IREpeI7xMDAOqMAxaoEb8av82gHP4BgAr+HE/hMntgjHmAEsxp8SvAIntjeo0IJMVM0NDTsDI4AXTTsK0AeYadrRc1RQEb5Th/Xmibc/4p0PYoys7Bp2OCISNCc2qZGj9zZx/s+BnwcWT5IgE9VcYLBpR/vjJ6gLgYvqX/RcKwZU1D837eiLwA+VpHLDyUN7URKBZxevazpRLZyh6j4RCVRgxRSea9UB3weeApYAMzzXWmTa0ZtMO3pOUN8r2dGtY1uqD+vOM0vWpz5JPf1EpBU02esvF6nXX+u5+6Xl/N+/3mWWfQr9urY/8AT19BPJG/nc60+dKSSlU4f7FX/PLVnX+Anq6ScirUCJKlOFvs9S0vUN/PMYvmXM45nFKRKVevqJSCtQospEod+TaeT6frznLipWPsYnnzaysaJ6+olIK1CiysRzN8Genfsf27PTP14IGrm+tvs+5bqSv/DK+410qRg5Ec6+3b8nRcT/ffbtqvoTkawKbcFvXir0ezIpruOQkk3cungdZx7Z98AX1dNPRAKmEVUmCv2eTIrr2NqmFy+8t566bG1VLyKSASWqTBT6PZkU17dq1A/ZsmMPC1ZvDScuESlqSlSZyLd7MplWKKa4vkGnXkpZaYRnU5Wpi4gESAt+C1V9BV9ycURZ+xYn1m/cO4e1W3fy/HXjshejiLQaLfjNZ2Guiwryu7NcoXjqsN6s2LCd5Ru2ZSE4EZH0FXeiCnNdVNDfneUKxfHDegFNdKkQEQlIcSeqpkYdQY+0gl6TleUKxco1jzOn/dV86/nRxKccUTiLnEUk5xV3oko56lgT/Egr6DVZ2axQTIz+esc3UEKcSKy2sDpyiEhOK+5ElWp0ESkNvgNF0GuyslmhWOgdOUQkpxV3Z4rxNzReGdfwL+V62exAkeq7s7kmK1tdI1JcdzxWizapF5GgFfeIKtWow6hq/PxsdqDIpzVZKa57Q0lP9qhbhYgErLhHVJB61BH0aKep7841jYz+6krL+X87z6fymaX8eMLhIQYnIoWuuEdUqbRktFPI+1Q18r9Hm3PvoMPoSdz14nJeWroh7AhFpICpM0U2ZLkLRL7YuXsv5905i43bdvHE1V+kd5fysEMSkRTUmaLYFWlVXPu2pdz5taPZsXsvV09/g3z7R4+I5Aclqmwo9H2qmjCoV2euP/NwXluxmddWbA47HBEpQEpU2ZCP+1Rl8Z7axOoqunYo4/7ZXvbiExFJUNVfNmR7TdTCGf60YazWT3bjb8juva6G99TqO29Ai76nvKyUC489lJEWOUMAAA1dSURBVKkvL6d2yw4qu3XIXqwi0joc4z7gLGA9TmxE4lh34C+ACXjARJzYFhwjAtwGnAnsAC7BiS0IKjSNqLKhqSrBTEcurdEoN4B7al8fcygAf35t9cFEJiLhmQZMaHDMBp7DiQ0Gnks8BzgDGJz4mQz8LsjAlKiyZeREuPYdcLb6v+uTVKZJpzUKMwK4p1bZrQOnD+/D9Lmr+XTP3hZ/joiExIm9DDS80XwucH/i8f3AeUnHH8CJxXFirwFdcYy+QYWmRBWkliSd1ijMCOie2sVjTbbu2MNjb649qM8Rkey77oS2FTjGvKSfyWm8rTdO7MPE44+A3onH/YA1SefVJo4FQveogtSSpGNUJkZgjRzPloD6DI4Z0J2hvTszbfYqJlZXEYmoE6BIrrjl1d0bb569q+XrqJxYHMcIZQ2KRlRBasnIJZvbc6QSUJ/BSCTCJTUmSz78mLneluzEKiJhWvfZlJ7/e33i+FoguSlqZeJYIJSogtSSpNNazWobu6eWBeeN6ofRvoxps1dm5fNEJFQzgYsTjy8GHks6/k0cI4JjjAFiSVOEWaepvyDV/+Wfaal5vjSrbUT7tqVccGwV9/57JR9s3ckhXds3/yYRCZ9jPAyMAypwjFrgRsAFZuAYlwOrgPq/mJ7AL01fhl+efmmQoanXn2Tdms07OOnXL/DdcQP50ZfUWV0kF6jXn0iSqu4dGD+sNw+/vkal6iJy0JSo8kkebSVyyViTzdt388+3Pgg7FBHJc4HeozLt6AT8NhulwD2ea7kNXv8B8C2gDtgAXOa51qogY8pbWW57FLSxA3swuFcnps32OH90pUrVRaTFAhtRmXa0FLgTv9XGcGCSaUeHNzjtDaDac62RwCPAr4KKJ+/l2VYikUiEi8eaLPrgY95Z+3HY4YhIHgty6u84YJnnWis819oNTMdvu/EZz7Ve8FxrR+Lpa/i1+NKYPNxK5EtH9AHglWXaAVhEWi7Iqb/GWmwc38T5lwP/auwF045Oxm98SMn23dmKL7+0RseKLOvZuR1De3dm9rJNfG/coLDDEZE8lRPrqEw7+nWgGjipsdc915oKTAWofvbG/Kqnz5aA2h4FbeygHjw0x29UW15WGnY4IpKHgpz6S6vFhmlHTwX+CzjHc61dAcaT31qrY0WW1QysYFfdPhasVkslEWmZIEdUc4HBph3tj5+gLgQuSj7BtKNHA3cDEzzXWn/gR8h+8rBjxfEDulNaEmH2sk2MHVgRdjgikocCG1F5rlUHfB94ClgCzPBca5FpR28y7eg5idN+DXQC/mra0TdNOzozqHgkHJ3LyxhZaTBr+cawQxGRPKUWShK4m596j9+9tJw3bziNzuVlYYcjUpTUQkmkCWMH9WDvvjivr2y4eaiISPOUqCRwxxzajXZtSpi1bFPYoYhIHlKiksCVl5VSbXZjtu5TiUgLKFEVgjxoVjt2YAXvfvQJG7dpBYKIZEaJKt/VN6uNrQHinzerzbFkVTPIL02fvVzTfyKSGSWqfJcnzWqP7GfQubwNs5dp+k9EMqNEle/ypFltaUmEMQN6aEQlIhlTosp3qZrS5mCz2pqBPVi9eQdrNu9o/mQRkQQlqnw3/ga/OW2yHG1W+/l9Kk3/iUj6lKjyXR41qx3UqxM9O7fTeioRyUhObPMhBylPmtVGIhHGDuzBrGWbiMfj2p5eRNKiEZW0qpqBFWzctoul67aFHYqI5AklKmlVYwf1AGCWytRFJE2a+pNWVdmtA4f16MDs5Ru57Av9ww5HROo5xrXAt4A48DZwKdAXmA70AOYD38CJ7W7t0DSiklY3dmAFc1Zspm7vvrBDEREAx+gHXAVU48RGAKX4m93+EpiCExsEbAEuDyM8JSppdTWDevDJrjreXhsLOxQR+VwboD2O0QboAHwInAI8knj9fuC8MAJTopJWN3ZgBWWlEe5+aQX5tnGnSEFyYmuBm4HV+Akqhj/VtxUnVpc4qxboF0Z4SlTS6rp3bMsPTx/Kk4s+4qHXV4cdjkhRuO6EthU4xrykn8mfvegY3YBzgf7AIUBHYEI4kR5IxRQSim9/cQCzlm/ipn8upvqw7gzt0znskEQK2i2v7t548+xdqbaiPxVYiRPbAIBjPArUAF1xjDaJUVUlsLZVgm1AIyoJRUlJhFu+ehSdy9tw5cML2Ll7b9ghiRSz1cAYHKMDjhEBxgOLgReA8xPnXAw8FkZwSlQSmp6d23HrxFEsXbeNn0UXhx2OSPFyYnPwiyYW4JemlwBTgZ8AP8AxluGXqN8bRniRfLuZXV1dHZ83b17YYUgW/d8TS7j75RX87mvHcMaRfcMOR6QgRSKR+fF4PNXUX04rjhFVHmzVXsyuO30oR1Ua/ORvC6ndoi1ARGR/hZ+o8mSr9mLWtk0Jd0w6hn1xuGb6m1oILCL7KfypvykjEkmqAaMKrn0ne4HJQXvszbVcPf1Nqrq3p7xNadjhiOScq8YP5uyjDmnRe/N56q/wy9PzZKt2gXNH9WPjtt3MX7U57FBEcpLRvizsEEJR+InKqEwxosq9rdoFLv9Cfy5Xs1oRSVL496jyaKt2ERE5UOEnqjzaql1ERA5U+FN/kDdbtYuIyIEKf0QlIiJ5TYlKRERymhKViIjkNCUqERHJaUpUIiKS0/KuhVIkEtkArGrqnJIOXSv27di6sZVCyhm67uJSrNcNxXvtB3ndh8Xj8Z5ZDai1xOPxgvs57CePzws7Bl23rlvXrWvXdWfnR1N/IiKS05SoREQkpxVqopoadgAh0XUXl2K9bijeay/K6867YgoRESkuhTqiEhGRAqFEJSIiOa3guqebdnQCcBtQCtzjuZYbckiBMO3ofcBZwHrPtUYkjnUH/gKYgAdM9FxrS1gxBsG0o1XAA0BvIA5M9VzrtkK/dtOOlgMvA+3w/7t9xHOtG0072h+YDvQA5gPf8Fxrd3iRBsO0o6XAPGCt51pnFcN1m3bUAz4B9gJ1nmtVF/qf81QKakSV+MN8J3AGMByYZNrR4eFGFZhpwIQGx2zgOc+1BgPPJZ4XmjrgOs+1hgNjgCsS/x8X+rXvAk7xXOsoYBQwwbSjY4BfAlM81xoEbAEuDzHGIF0NLEl6XizXfbLnWqM816pOPC/0P+eNKqhEBRwHLPNca0XiX1fTgXNDjikQnmu9DGxucPhc4P7E4/uB81o1qFbgudaHnmstSDz+BP8vr34U+LV7rhX3XGtb4mlZ4icOnAI8kjhecNcNYNrRSsAC7kk8j1AE151CQf85T6XQpv76AWuSntcCx4cUSxh6e671YeLxR/jTYwXLtKMmcDQwhyK49sSMwXxgEP7MwXJgq+dadYlTavH/Gyg0vwF+DHROPO9BcVx3HHjatKNx4G7PtaZSBH/OG1NoIypJ8Fwrjv8HvSCZdrQT8DfgGs+1Pk5+rVCv3XOtvZ5rjQIq8WcPDg85pMCZdrT+Puz8sGMJwRc81zoG/1bGFaYdPTH5xUL9c96YQktUa4GqpOeViWPFYp1pR/sCJH6vDzmeQJh2tAw/ST3oudajicNFce0AnmttBV4ATgC6mna0fmakEP+81wDnJAoLpuNP+d1G4V83nmutTfxeD/wd/x8nRfPnPFmhJaq5wGDTjvY37Whb4EJgZsgxtaaZwMWJxxcDj4UYSyAS9yfuBZZ4rnVr0ksFfe2mHe1p2tGuicftgdPw78+9AJyfOK3grttzres916r0XMvE/+/5ec+1vkaBX7dpRzuadrRz/WPgdOAdCvzPeSoFdY/Kc606045+H3gKvzz9Ps+1FoUcViBMO/owMA6oMO1oLXAj4AIzTDt6Of5WKBPDizAwNcA3gLdNO/pm4thPKfxr7wvcn7hPVQLM8FzrcdOOLgamm3b0/wFv4CfxYvATCvu6ewN/N+0o+H9PP+S51pOmHZ1LYf85b5RaKImISE4rtKk/EREpMEpUIiKS05SoREQkpylRiYhITlOiEhGRnFZQ5eki2WLa0d7AFPzGt1uA3cCvPNf6e6iBiRQhjahEGkgsKv4H8LLnWgM81xqNv9i0MtzIRIqT1lGJNGDa0fHADZ5rndTIaybwJ6Bj4tD3PdeabdrRccD/AluBI4EZwNv421O0B87zXGu5aUd7Ar8HDk28/xrPtWYFeDkieU8jKpEDHQEsSPHaeuC0RLPQC4Dbk147CvhPYBh+94whnmsdh789xZWJc27D30fpWOAriddEpAm6RyXSDNOO3gl8Af8+1anAb007Ogp/59UhSafOrd+CwbSjy4GnE8ffBk5OPD4VGJ5ojQPQxbSjnZL2mhKRBpSoRA60CH+0A4DnWleYdrQCfyv0a4F1+KOnEuDTpPftSnq8L+n5Pj7/b60EGOO5VvL7RKQJmvoTOdDzQLlpR7+bdKxD4rcBfOi51j786b3SDD/7aT6fBiQxMhORJmhEJdKA51px046eB0wx7eiPgQ3AdvyO3QuAv5l29JvAk4njmbgKuNO0owvx//t7Gf++loikoKo/ERHJaZr6ExGRnKZEJSIiOU2JSkREcpoSlYiI5DQlKhERyWlKVCIiktOUqEREJKf9fy3AaWEuwXtWAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX4Ope7lLuSz"
      },
      "source": [
        "Population Based Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edIN-t4WQgpn"
      },
      "source": [
        "TARGET_UPDATE = 10\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IgzpaQh5GTGH",
        "outputId": "9e53b674-91aa-4665-c5bf-b0765ad85e23"
      },
      "source": [
        "    env = gym.make('SpaceInvaders-v0')\n",
        "\n",
        "\n",
        "    scores = []\n",
        "    #epsHistory = []\n",
        "    numGames = 4\n",
        "    batch_size= 32\n",
        "    # uncomment the line below to record every episode.\n",
        "    env = wrappers.Monitor(env, \"tmp/space-invaders-1\", video_callable=lambda episode_id: True, force=True)\n",
        "    no_agents = 15\n",
        "    generations = 3\n",
        "    best_parent = None\n",
        "    best_score = 0\n",
        "\n",
        "    for g in range(generations):\n",
        "        print('Training for generation:', g)\n",
        "        for a in range(no_agents):\n",
        "            total_score = 0\n",
        "            agent_score = []\n",
        "            epsHistory = []\n",
        "\n",
        "            brain = Agent(gamma=0.95, epsilon=1.0,\n",
        "                  alpha=0.003, maxMemorySize=5000,\n",
        "                  replace=None)\n",
        "            while brain.memCntr < brain.memSize:\n",
        "                observation = env.reset()\n",
        "                done = False\n",
        "                while not done:\n",
        "            # 0 no action, 1 fire, 2 move right, 3 move left, 4 move right fire, 5 move left fire\n",
        "                    action = env.action_space.sample()\n",
        "                    observation_, reward, done, info = env.step(action)\n",
        "                    if done and info['ale.lives'] == 0:\n",
        "                        reward = -100\n",
        "                    brain.storeTransition(np.mean(observation[15:200,30:125], axis=2), action, reward,\n",
        "                                np.mean(observation_[15:200,30:125], axis=2))\n",
        "                    observation = observation_\n",
        "            print('done initializing memory for agent ', a)\n",
        "            #print('best parent', best_parent)\n",
        "\n",
        "            for i in range(numGames):\n",
        "                print('starting game ', i+1, 'epsilon: %.4f' % brain.EPSILON)\n",
        "                epsHistory.append(brain.EPSILON)\n",
        "                done = False\n",
        "                observation = env.reset()\n",
        "                frames = [np.sum(observation[15:200,30:125], axis=2)]\n",
        "                score = 0\n",
        "                lastAction = 0\n",
        "                while not done:\n",
        "                    print(\"len\",len(frames))\n",
        "                    if len(frames) == 3:\n",
        "                        action = brain.chooseActionPBT(frames, best_parent)\n",
        "                        frames = []\n",
        "                    else:\n",
        "                        action = lastAction\n",
        "                    observation_, reward, done, info = env.step(action)\n",
        "                    score += reward\n",
        "                    frames.append(np.sum(observation_[15:200,30:125], axis=2))\n",
        "                    if done and info['ale.lives'] == 0:\n",
        "                        reward = -100\n",
        "                    brain.storeTransition(np.mean(observation[15:200,30:125], axis=2), action, reward,\n",
        "                                  np.mean(observation_[15:200,30:125], axis=2))\n",
        "                    observation = observation_\n",
        "                    brain.learn(batch_size)\n",
        "                    lastAction = action\n",
        "                total_score += score\n",
        "\n",
        "                if i % TARGET_UPDATE == 0:\n",
        "                    brain.Q_next.load_state_dict(brain.Q_eval.state_dict())\n",
        "                  \n",
        "                #env.render()\n",
        "                scores.append(score)\n",
        "                agent_score.append(score)\n",
        "                print('score:',score)\n",
        "            #x = [i+1 for i in range(numGames)]\n",
        "            #fileName = str(numGames) + 'Games' + 'Gamma' + str(brain.GAMMA) + \\\n",
        "             #      'Alpha' + str(brain.ALPHA) + 'Memory' + str(brain.memSize)+ '.png'\n",
        "            #plotLearning(x, agent_score, epsHistory, fileName)\n",
        "            if total_score/(numGames) > best_score: #average score\n",
        "                print(\"Found best in current generation\", total_score/numGames)\n",
        "                best_score = total_score/numGames\n",
        "                best_curr = brain.Q_eval\n",
        "        best_parent = best_curr\n",
        "    print('Best Average Score:', best_score)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for generation: 0\n",
            "done initializing memory for agent  0\n",
            "starting game  1 epsilon: 1.0000\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n",
            "len 3\n",
            "len 1\n",
            "len 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4c2a567d16d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m                               np.mean(observation_[15:200,30:125], axis=2))\n\u001b[1;32m     62\u001b[0m                 \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mlastAction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mtotal_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-091e06f0e266>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# convert to list because memory is an array of numpy objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mQpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mQnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-091e06f0e266>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m#observation = observation.view(-1, 3, 210, 160).to(self.device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m185\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eevTNOYFLhmI"
      },
      "source": [
        "Baseline Approach: Shared Buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuMpAiIrPXGN"
      },
      "source": [
        "TARGET_UPDATE = 10\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPnASXO6VWq6"
      },
      "source": [
        "class Agent_Baseline(object):\n",
        "    def __init__(self, gamma, epsilon, alpha,\n",
        "                 maxMemorySize, epsEnd=0.05,\n",
        "                 replace=10000, actionSpace=[0,1,2,3,4,5]):\n",
        "        self.GAMMA = gamma\n",
        "        self.EPSILON = epsilon\n",
        "        self.EPS_END = epsEnd\n",
        "        self.ALPHA = alpha\n",
        "        self.actionSpace = actionSpace\n",
        "        self.memSize = maxMemorySize\n",
        "        self.steps = 0\n",
        "        self.learn_step_counter = 0\n",
        "        self.memory = []\n",
        "        self.memCntr = 0\n",
        "        self.replace_target_cnt = replace\n",
        "        self.Q_eval = DeepQNetwork(alpha)\n",
        "        self.Q_next = DeepQNetwork(alpha)\n",
        "        self.score = 0\n",
        "        self.env = gym.make('SpaceInvaders-v0').unwrapped\n",
        "        self.lastAction = 0\n",
        "        self.observation = None\n",
        "        self.done = False\n",
        "        self.frames = []\n",
        "\n",
        "\n",
        "    def storeTransition(self, state, action, reward, state_):\n",
        "        if self.memCntr < self.memSize:\n",
        "            self.memory.append([state, action, reward, state_])\n",
        "        else:\n",
        "            self.memory[self.memCntr%self.memSize] = [state, action, reward, state_]\n",
        "        self.memCntr += 1\n",
        "\n",
        "    def chooseAction(self, observation):\n",
        "        rand = np.random.random()\n",
        "        actions = self.Q_eval.forward(observation)\n",
        "        if rand < 1 - self.EPSILON:\n",
        "            action = T.argmax(actions[1]).item()\n",
        "        else:\n",
        "            action = np.random.choice(self.actionSpace)\n",
        "        self.steps += 1\n",
        "        return action\n",
        "\n",
        "    def chooseActionPBT(self, observation, policy_net_parent=None):\n",
        "        rand = np.random.random()\n",
        "        eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * self.steps / EPS_DECAY)\n",
        "        self.steps += 1\n",
        "        if rand > eps_threshold:\n",
        "            return T.argmax(self.Q_eval.forward(observation)[1]).item() #self.Q_eval.forward(observation).max(1)[1].view(1, 1)\n",
        "        else:\n",
        "            if policy_net_parent and random.random() < 0.5:\n",
        "                return T.argmax(policy_net_parent(observation)[1]).item() #policy_net_parent(observation).max(1)[1].view(1, 1)\n",
        "            else:\n",
        "                return np.random.choice(self.actionSpace) #T.tensor([[random.randrange(self.actionSpace)]], device=device, dtype=T.long)\n",
        "\n",
        "    def learn(self, batch_size):\n",
        "        self.Q_eval.optimizer.zero_grad()\n",
        "        if self.replace_target_cnt is not None and \\\n",
        "           self.learn_step_counter % self.replace_target_cnt == 0:\n",
        "            self.Q_next.load_state_dict(self.Q_eval.state_dict())\n",
        "\n",
        "        #print(self.memCntr, batch_size, self.memSize)\n",
        "        if self.memCntr+batch_size < self.memSize:\n",
        "            memStart = int(np.random.choice(range(self.memCntr)))\n",
        "        else:\n",
        "            memStart = int(np.random.choice(range(self.memSize-batch_size-1)))\n",
        "        miniBatch=self.memory[memStart:memStart+batch_size]\n",
        "        memory = np.array(miniBatch)\n",
        "\n",
        "        # convert to list because memory is an array of numpy objects\n",
        "        Qpred = self.Q_eval.forward(list(memory[:,0][:])).to(self.Q_eval.device)\n",
        "        Qnext = self.Q_next.forward(list(memory[:,3][:])).to(self.Q_eval.device)\n",
        "\n",
        "        maxA = T.argmax(Qnext, dim=1).to(self.Q_eval.device)\n",
        "        rewards = T.Tensor(list(memory[:,2])).to(self.Q_eval.device)\n",
        "        Qtarget = Qpred.clone()\n",
        "        indices = np.arange(batch_size)\n",
        "        Qtarget[indices,maxA] = rewards + self.GAMMA*T.max(Qnext[1])\n",
        "\n",
        "        if self.steps > 500:\n",
        "            if self.EPSILON - 1e-4 > self.EPS_END:\n",
        "                self.EPSILON -= 1e-4\n",
        "            else:\n",
        "                self.EPSILON = self.EPS_END\n",
        "\n",
        "        #Qpred.requires_grad_()\n",
        "        loss = self.Q_eval.loss(Qtarget, Qpred).to(self.Q_eval.device)\n",
        "        loss.backward()\n",
        "        self.Q_eval.optimizer.step()\n",
        "        self.learn_step_counter += 1\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v0OrLrbLnuI",
        "outputId": "216d30d8-9e60-4f88-bca1-e4ee1dfb6f91"
      },
      "source": [
        "    env = gym.make('SpaceInvaders-v0')\n",
        "\n",
        "\n",
        "    scores = []\n",
        "    #epsHistory = []\n",
        "    numGames = 2\n",
        "    batch_size= 32\n",
        "    # uncomment the line below to record every episode.\n",
        "    env = wrappers.Monitor(env, \"tmp/space-invaders-1\", video_callable=lambda episode_id: True, force=True)\n",
        "    no_agents = 2\n",
        "    generations = 2\n",
        "    best_parent = None\n",
        "    best_score = 0\n",
        "\n",
        "    for g in range(generations):\n",
        "        print('Training for generation:', g)\n",
        "        total_score = 0\n",
        "        agent_score = []\n",
        "        epsHistory = []\n",
        "\n",
        "        agents = [Agent_Baseline(gamma=0.95, epsilon=1.0,\n",
        "              alpha=0.003, maxMemorySize=5000,\n",
        "              replace=None) for _ in range(no_agents)]\n",
        "\n",
        "        brain = agents[0] #shared memory\n",
        "        while brain.memCntr < brain.memSize:\n",
        "            observation = brain.env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "            # 0 no action, 1 fire, 2 move right, 3 move left, 4 move right fire, 5 move left fire\n",
        "                action = env.action_space.sample()\n",
        "                observation_, reward, done, info = brain.env.step(action)\n",
        "                if done and info['ale.lives'] == 0:\n",
        "                    reward = -100\n",
        "                brain.storeTransition(np.mean(observation[15:200,30:125], axis=2), action, reward,\n",
        "                            np.mean(observation_[15:200,30:125], axis=2))\n",
        "                observation = observation_\n",
        "        print('done initializing shared memory for agents ')\n",
        "        #print('best parent', best_parent)\n",
        "  \n",
        "        for i in range(numGames):\n",
        "            print('starting game ', i+1)\n",
        "            for a in agents:\n",
        "              a.observation = a.env.reset()\n",
        "              a.frames = [np.sum(a.observation[15:200,30:125], axis=2)]\n",
        "\n",
        "            running = [1]*no_agents\n",
        "\n",
        "            while sum(running) > 1:\n",
        "                for j, a in enumerate(agents):\n",
        "                  if running[j]:\n",
        "                    epsHistory.append(a.EPSILON)\n",
        "                    if len(a.frames) == 3:\n",
        "                        action = a.chooseActionPBT(a.frames, best_parent)\n",
        "                        a.frames = []\n",
        "                    else:\n",
        "                        action = a.lastAction\n",
        "                    observation_, reward, a.done, info = a.env.step(action)\n",
        "                    #print(action, a.observation, reward, a.done, info)\n",
        "                    a.score += reward\n",
        "\n",
        "                    a.frames.append(np.sum(observation_[15:200,30:125], axis=2))\n",
        "                    if a.done and info['ale.lives'] == 0:\n",
        "                      reward = -100\n",
        "                    if a.done:\n",
        "                      running[j] = 0\n",
        "                      break\n",
        "                    brain.storeTransition(np.mean(a.observation[15:200,30:125], axis=2), action, reward,\n",
        "                              np.mean(observation_[15:200,30:125], axis=2))\n",
        "                    a.memory = brain.memory\n",
        "                    a.observation = observation_\n",
        "                    brain.learn(batch_size)\n",
        "                    a.lastAction = action\n",
        "\n",
        "            if i % TARGET_UPDATE == 0:\n",
        "              for a in agents:\n",
        "                a.Q_next.load_state_dict(a.Q_eval.state_dict())\n",
        "          \n",
        "        max_score = -1\n",
        "        best_a = None\n",
        "        for a in agents:\n",
        "          if a.score > max_score:\n",
        "            max_score = a.score\n",
        "            best_a = a\n",
        "\n",
        "        #print(max_score, numGames, best_score)\n",
        "        if max_score/numGames > best_score: #average score\n",
        "            print(\"Found best in current generation\", max_score/numGames)\n",
        "            best_score = max_score/numGames\n",
        "            best_curr = best_a.Q_eval\n",
        "        best_parent = best_curr\n",
        "    print('Best Average Score:', best_score)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for generation: 0\n",
            "done initializing shared memory for agents \n",
            "starting game  1\n",
            "starting game  2\n",
            "530.0 2 0\n",
            "Found best in current generation 265.0\n",
            "Training for generation: 1\n",
            "done initializing shared memory for agents \n",
            "starting game  1\n",
            "starting game  2\n",
            "400.0 2 265.0\n",
            "Best Average Score: 265.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-8i57mWPut2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}